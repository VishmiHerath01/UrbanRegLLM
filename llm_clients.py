import openai
from transformers import pipeline
from huggingface_hub import InferenceClient

# OpenAI API client class
class OpenAIClient:
    """Client for interacting with OpenAI's API."""
    def __init__(self, api_key: str):
        """Initializes the OpenAI client with the API key."""
        openai.api_key = api_key
        # Use the new client initialization for openai >= 1.0.0
        self.client = openai.OpenAI(api_key=api_key)
        print("OpenAI Client initialized.")

    def generate(self, prompt: str, max_tokens: int = 150) -> str:
        """Generates text using the OpenAI API (updated for openai >= 1.0.0)."""
        try:
            # Use the new chat completions endpoint
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo", # Or another suitable chat model
                messages=[
                    {"role": "system", "content": "You are a helpful assistant answering questions based on provided context."}, 
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens
            )
            # Extract the message content from the response
            if response.choices and response.choices[0].message:
                return response.choices[0].message.content.strip()
            else:
                return "Error: No response generated by OpenAI."
        except Exception as e:
            print(f"Error generating text with OpenAI API: {e}")
            return "Error generating response from OpenAI."


# HuggingFace API client class (for models hosted on HuggingFace Hub)
class HuggingFaceClient:
    """Client for interacting with Hugging Face models via the Inference API."""

    def __init__(self, model_name: str, api_token: str):
        """Initializes the Hugging Face Inference API client.

        Args:
            model_name (str): The name of the model to use (e.g., 'mistralai/Mistral-7B-Instruct-v0.3').
            api_token (str): Your Hugging Face API token.
        """
        self.model_name = model_name
        # Use InferenceClient with the provided token
        self.client = InferenceClient(token=api_token) 
        print(f"HuggingFace Inference API Client initialized for model: {self.model_name}")

    def generate(self, prompt: str, max_new_tokens: int = 150) -> str:
        """Generates text using the Hugging Face Inference API.

        Args:
            prompt (str): The input prompt for the model.
            max_new_tokens (int): The maximum number of new tokens to generate.

        Returns:
            str: The generated text.
        """
        # Check if prompt is too long and truncate if necessary
        max_prompt_tokens = 1500  # Safe limit for most models
        if len(prompt.split()) > max_prompt_tokens:
            print(f"Warning: Prompt too long ({len(prompt.split())} tokens), truncating to {max_prompt_tokens} tokens")
            prompt_words = prompt.split()
            # Keep the first part and the last part to preserve context and question
            first_part = prompt_words[:max_prompt_tokens // 2]
            last_part = prompt_words[-max_prompt_tokens // 2:]
            prompt = ' '.join(first_part + last_part)
            
        try:
            # Call the Inference API with minimal parameters
            response = self.client.text_generation(
                prompt,
                model=self.model_name,
                max_new_tokens=max_new_tokens,
                do_sample=True,  # Use sampling instead of greedy decoding
                temperature=0.7   # Control randomness
            )
            return response.strip()
        except Exception as e:
            print(f"Error generating text with Hugging Face API for model {self.model_name}: {e}")
            # Try alternative parameters if the first attempt fails
            try:
                print("Attempting with alternative parameters...")
                # Use even more minimal parameters
                response = self.client.text_generation(
                    prompt,
                    model=self.model_name,
                    max_new_tokens=max_new_tokens
                )
                return response.strip()
            except Exception as e2:
                print(f"Second attempt also failed: {e2}")
                return "Error generating response from HuggingFace API."