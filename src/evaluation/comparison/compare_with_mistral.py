import os
import json
import csv
import time
import logging
import sys
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm

# Add the src directory to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Import functions from your Mistral pipelines
from rag_pipelines.pretrained.mistralWpretrained_em import rag_pipeline as regular_rag_pipeline

# Set up logging
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO)

# Configuration
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))), "data")
RESULTS_DIR = os.path.join(DATA_DIR, "comparison_results")
os.makedirs(RESULTS_DIR, exist_ok=True)

# Model paths
REGULAR_EMBEDDING_MODEL = "/Users/vishmiherath/Documents/FYP/legal-embeddings-model"
MRL_EMBEDDING_MODEL = "/Users/vishmiherath/Documents/FYP/data/my-mrl-embeddings"
MRL_DIMENSION = 256  # Default MRL dimension to use

def load_test_queries():
    """Load test queries from test.json"""
    test_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))), "/Users/vishmiherath/Documents/FYP/data/datasets/splits/test.json")
    
    if not os.path.exists(test_file):
        logging.error(f"Test file not found at {test_file}")
        return []
    
    try:
        with open(test_file, 'r') as f:
            test_data = json.load(f)
        
        # Extract just the questions
        queries = [item['question'] for item in test_data]
        
        # Limit to a smaller set for testing if needed
        # return queries[:10]
        return queries
    except Exception as e:
        logging.error(f"Error loading test queries: {str(e)}")
        return []

def get_context_from_results(docs):
    """Extract context from retrieved documents"""
    if not docs:
        return ""
    
    context = ""
    for i, doc in enumerate(docs):
        context += f"Document {i+1}:\n{doc['content']}\n\n"
    
    return context

def compare_with_mistral():
    """Compare retrieval approaches by generating answers with Mistral"""
    # Load test queries
    test_queries = load_test_queries()
    if not test_queries:
        logging.error("No test queries found.")
        return
    
    logging.info(f"Loaded {len(test_queries)} test queries")
    
    # Prepare for results
    answers = {}
    
    # Generate answers for each query
    for query in tqdm(test_queries, desc="Processing queries"):
        logging.info(f"Generating answers for query: {query}")
        
        # Generate answer using regular embeddings
        try:
            regular_answer, regular_docs, regular_info = regular_rag_pipeline(
                query,
                embedding_model_path=REGULAR_EMBEDDING_MODEL,
                k=5
            )
            regular_context = get_context_from_results(regular_docs)
            logging.info(f"Generated answer with regular embeddings: {len(regular_answer)} chars")
        except Exception as e:
            logging.error(f"Error with regular embeddings: {str(e)}")
            regular_answer = f"Error: {str(e)}"
            regular_context = ""
            regular_docs = []
        
        # Store the results
        answers[query] = {
            "regular": {
                "answer": regular_answer,
                "context": regular_context,
                "metrics": regular_info
            }
        }
    
    # Save the answers to a JSON file
    answers_file = os.path.join(RESULTS_DIR, "mistral_answers.json")
    with open(answers_file, 'w') as f:
        json.dump(answers, f, indent=2)
    
    # Generate CSV for easier analysis
    csv_file = os.path.join(RESULTS_DIR, "mistral_comparison.csv")
    with open(csv_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Query", "Regular Answer", "Regular Search Time", "Regular Tokens"])
        
        for query, data in answers.items():
            writer.writerow([
                query,
                data["regular"]["answer"],
                data["regular"]["metrics"].get("search_time", "N/A") if data["regular"]["metrics"] else "N/A",
                data["regular"]["metrics"].get("tokens", "N/A") if data["regular"]["metrics"] else "N/A"
            ])
    
    # Generate a markdown report
    generate_markdown_report(answers)
    
    logging.info(f"Results saved to {answers_file} and {csv_file}")
    return answers

def generate_markdown_report(answers):
    """Generate a detailed markdown report comparing the answers"""
    report_path = os.path.join(RESULTS_DIR, "mistral_answer_report.md")
    
    with open(report_path, 'w') as f:
        f.write("# Mistral Answer Report\n\n")
        
        f.write("## Overview\n\n")
        f.write("This report shows answers generated by Mistral using regular legal embeddings.\n\n")
        
        # Performance metrics
        f.write("## Performance Metrics\n\n")
        
        # Calculate average metrics
        regular_search_times = [data["regular"]["metrics"].get("search_time", 0) for data in answers.values() if data["regular"]["metrics"]]
        regular_tokens = [data["regular"]["metrics"].get("tokens", 0) for data in answers.values() if data["regular"]["metrics"]]
        
        avg_regular_search_time = np.mean(regular_search_times) if regular_search_times else "N/A"
        avg_regular_tokens = np.mean(regular_tokens) if regular_tokens else "N/A"
        
        f.write("### Average Metrics\n\n")
        f.write("| Metric | Regular Embeddings |\n")
        f.write("|--------|-------------------|\n")
        f.write(f"| Search Time | {avg_regular_search_time:.4f}s |\n")
        f.write(f"| Tokens Used | {avg_regular_tokens:.1f} |\n\n")
        
        # Detailed results for each query
        f.write("## Detailed Results\n\n")
        
        for query, data in answers.items():
            f.write(f"### Query: \"{query}\"\n\n")
            
            # Regular embeddings
            f.write("#### Regular Embeddings\n\n")
            f.write("**Answer:**\n\n")
            f.write(f"```\n{data['regular']['answer']}\n```\n\n")
            
            if data["regular"]["metrics"]:
                search_time = data["regular"]["metrics"].get("search_time", "N/A")
                tokens = data["regular"]["metrics"].get("tokens", "N/A")
                f.write(f"**Metrics:** Search Time: {search_time:.4f}s, Tokens: {tokens}\n\n")
            
            f.write("---\n\n")
    
    logging.info(f"Markdown report generated at {report_path}")

if __name__ == "__main__":
    compare_with_mistral()
