import os
import json
import logging
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from bert_score import score as bert_score
import torch
from tqdm import tqdm

# Set up logging
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO)

# Configuration
DATA_DIR = "data"
RESULTS_DIR = os.path.join(DATA_DIR, "comparison_results")
os.makedirs(RESULTS_DIR, exist_ok=True)

def load_test_data():
    """Load test data with ground truth answers"""
    test_file = "test.json"
    
    if not os.path.exists(test_file):
        logging.error(f"Test file not found at {test_file}")
        return None
    
    with open(test_file, 'r') as f:
        test_data = json.load(f)
    
    # Create a dictionary mapping questions to answers
    qa_pairs = {item['question']: item['answer'] for item in test_data}
    
    return qa_pairs

def load_mistral_answers():
    """Load the answers generated by Mistral"""
    answers_file = os.path.join(RESULTS_DIR, "mistral_answers.json")
    
    if not os.path.exists(answers_file):
        logging.error(f"Answers file not found at {answers_file}")
        return None
    
    with open(answers_file, 'r') as f:
        answers = json.load(f)
    
    return answers

def calculate_rouge_scores(generated_answer, reference_answer):
    """Calculate ROUGE scores for a generated answer"""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference_answer, generated_answer)
    
    return {
        'rouge1_f': scores['rouge1'].fmeasure,
        'rouge2_f': scores['rouge2'].fmeasure,
        'rougeL_f': scores['rougeL'].fmeasure
    }

def calculate_bleu_score(generated_answer, reference_answer):
    """Calculate BLEU score for a generated answer"""
    # Tokenize the sentences
    reference_tokens = [reference_answer.split()]
    generated_tokens = generated_answer.split()
    
    # Apply smoothing to handle zero counts
    smoothing = SmoothingFunction().method1
    
    # Calculate BLEU score
    try:
        bleu_score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smoothing)
        return bleu_score
    except Exception as e:
        logging.error(f"Error calculating BLEU score: {str(e)}")
        return 0.0

def calculate_bert_score(generated_answers, reference_answers):
    """Calculate BERTScore for a batch of generated answers"""
    try:
        # BERTScore works better in batch mode
        P, R, F1 = bert_score(generated_answers, reference_answers, lang="en", rescale_with_baseline=True)
        return P.tolist(), R.tolist(), F1.tolist()
    except Exception as e:
        logging.error(f"Error calculating BERTScore: {str(e)}")
        # Return zeros with the same length as inputs
        zeros = [0.0] * len(generated_answers)
        return zeros, zeros, zeros

def evaluate_answers():
    """Evaluate the quality of generated answers"""
    # Load test data with ground truth answers
    qa_pairs = load_test_data()
    
    # Load generated answers
    mistral_answers = load_mistral_answers()
    if not mistral_answers:
        return
    
    # Prepare for evaluation
    evaluation_results = []
    
    # Lists for direct comparison between regular and MRL embeddings
    regular_answers = []
    mrl_answers = []
    queries = []
    has_reference = []
    reference_answers = []
    
    # Process each query
    for query, answers in mistral_answers.items():
        regular_answer = answers.get('regular_answer', '')
        mrl_answer = answers.get('mrl_answer', '')
        
        # Skip if both answers contain error messages
        if (regular_answer.startswith("Error:") and mrl_answer.startswith("Error:")):
            logging.warning(f"Skipping query with errors in both approaches: {query}")
            continue
        
        # Check if we have a ground truth answer
        if qa_pairs and query in qa_pairs:
            reference_answer = qa_pairs[query]
            has_reference.append(True)
            reference_answers.append(reference_answer)
        else:
            # No ground truth, but we can still compare the two approaches directly
            has_reference.append(False)
            reference_answers.append(None)
        
        # Add to lists for comparison
        regular_answers.append(regular_answer)
        mrl_answers.append(mrl_answer)
        queries.append(query)
    
    if not regular_answers:
        logging.error("No valid answers found for evaluation")
        return
    
    # Calculate BERTScores for direct comparison between regular and MRL
    # This compares how semantically similar the answers are to each other
    direct_P, direct_R, direct_F1 = calculate_bert_score(regular_answers, mrl_answers)
    
    # For queries with ground truth, calculate reference-based metrics
    ref_indices = [i for i, has_ref in enumerate(has_reference) if has_ref]
    
    if ref_indices:
        # Extract answers with references
        ref_regular_answers = [regular_answers[i] for i in ref_indices]
        ref_mrl_answers = [mrl_answers[i] for i in ref_indices]
        ref_reference_answers = [reference_answers[i] for i in ref_indices]
        ref_queries = [queries[i] for i in ref_indices]
        
        # Calculate BERTScores against reference
        regular_P, regular_R, regular_F1 = calculate_bert_score(ref_regular_answers, ref_reference_answers)
        mrl_P, mrl_R, mrl_F1 = calculate_bert_score(ref_mrl_answers, ref_reference_answers)
    
    # Calculate individual metrics for each query
    for i, query in enumerate(queries):
        result = {'query': query, 'has_reference': has_reference[i]}
        
        # Add direct comparison metrics
        result['similarity_score'] = direct_F1[i]  # How similar the two answers are to each other
        
        # Add reference-based metrics if available
        if has_reference[i]:
            idx = ref_indices.index(i)  # Find position in reference arrays
            reference_answer = reference_answers[i]
            regular_answer = regular_answers[i]
            mrl_answer = mrl_answers[i]
            
            # Calculate ROUGE scores
            regular_rouge = calculate_rouge_scores(regular_answer, reference_answer)
            mrl_rouge = calculate_rouge_scores(mrl_answer, reference_answer)
            
            # Calculate BLEU scores
            regular_bleu = calculate_bleu_score(regular_answer, reference_answer)
            mrl_bleu = calculate_bleu_score(mrl_answer, reference_answer)
            
            # Add to result
            result.update({
                'regular_rouge1': regular_rouge['rouge1_f'],
                'regular_rouge2': regular_rouge['rouge2_f'],
                'regular_rougeL': regular_rouge['rougeL_f'],
                'regular_bleu': regular_bleu,
                'regular_bert_precision': regular_P[idx],
                'regular_bert_recall': regular_R[idx],
                'regular_bert_f1': regular_F1[idx],
                'mrl_rouge1': mrl_rouge['rouge1_f'],
                'mrl_rouge2': mrl_rouge['rouge2_f'],
                'mrl_rougeL': mrl_rouge['rougeL_f'],
                'mrl_bleu': mrl_bleu,
                'mrl_bert_precision': mrl_P[idx],
                'mrl_bert_recall': mrl_R[idx],
                'mrl_bert_f1': mrl_F1[idx]
            })
        
        # Add length comparison
        result['regular_length'] = len(regular_answers[i])
        result['mrl_length'] = len(mrl_answers[i])
        result['length_difference'] = len(mrl_answers[i]) - len(regular_answers[i])
        
        # Store results
        evaluation_results.append(result)
    
    # Create DataFrame for analysis
    df = pd.DataFrame(evaluation_results)
    
    # Calculate average scores
    avg_scores = {
        'regular_rouge1': df['regular_rouge1'].mean(),
        'regular_rouge2': df['regular_rouge2'].mean(),
        'regular_rougeL': df['regular_rougeL'].mean(),
        'regular_bleu': df['regular_bleu'].mean(),
        'regular_bert_f1': df['regular_bert_f1'].mean(),
        'mrl_rouge1': df['mrl_rouge1'].mean(),
        'mrl_rouge2': df['mrl_rouge2'].mean(),
        'mrl_rougeL': df['mrl_rougeL'].mean(),
        'mrl_bleu': df['mrl_bleu'].mean(),
        'mrl_bert_f1': df['mrl_bert_f1'].mean()
    }
    
    # Save detailed results to CSV
    csv_file = os.path.join(RESULTS_DIR, "answer_metrics.csv")
    df.to_csv(csv_file, index=False)
    logging.info(f"Detailed metrics saved to {csv_file}")
    
    # Generate summary report
    generate_summary_report(df, avg_scores)
    
    # Generate visualizations
    generate_visualizations(df, avg_scores)
    
    return df, avg_scores

def generate_summary_report(df, avg_scores):
    """Generate a summary report of the metrics"""
    report_file = os.path.join(RESULTS_DIR, "answer_metrics_summary.md")
    
    with open(report_file, 'w') as f:
        f.write("# Answer Quality Metrics Summary\n\n")
        
        # Count queries with and without references
        with_ref = df[df['has_reference'] == True]
        without_ref = df[df['has_reference'] == False]
        
        f.write(f"Total Queries Analyzed: {len(df)}\n")
        f.write(f"- With Reference Answers: {len(with_ref)}\n")
        f.write(f"- Without Reference Answers: {len(without_ref)}\n\n")
        
        # Direct comparison section
        f.write("## Direct Comparison (All Queries)\n\n")
        
        # Answer length comparison
        avg_regular_length = df['regular_length'].mean()
        avg_mrl_length = df['mrl_length'].mean()
        length_diff = avg_mrl_length - avg_regular_length
        length_percent = (length_diff / avg_regular_length) * 100 if avg_regular_length > 0 else 0
        
        f.write("### Answer Length\n\n")
        f.write(f"- Regular Embeddings: {avg_regular_length:.1f} characters\n")
        f.write(f"- MRL Embeddings: {avg_mrl_length:.1f} characters\n")
        f.write(f"- Difference: {length_diff:.1f} characters ({length_percent:.1f}%)\n\n")
        
        # Similarity between answers
        avg_similarity = df['similarity_score'].mean()
        f.write("### Answer Similarity\n\n")
        f.write(f"Average semantic similarity between regular and MRL answers: {avg_similarity:.4f}\n\n")
        
        # Only show reference-based metrics if we have reference answers
        if not with_ref.empty:
            f.write("## Reference-Based Metrics\n\n")
            f.write("| Metric | Regular Embeddings | MRL Embeddings | Difference | % Improvement |\n")
            f.write("|--------|-------------------|----------------|------------|---------------|\n")
            
            # Calculate average scores for queries with references
            ref_metrics = {}
            for metric in ['rouge1', 'rouge2', 'rougeL', 'bleu', 'bert_f1']:
                if f'regular_{metric}' in with_ref.columns and f'mrl_{metric}' in with_ref.columns:
                    ref_metrics[f'regular_{metric}'] = with_ref[f'regular_{metric}'].mean()
                    ref_metrics[f'mrl_{metric}'] = with_ref[f'mrl_{metric}'].mean()
            
            for metric in ['rouge1', 'rouge2', 'rougeL', 'bleu', 'bert_f1']:
                if f'regular_{metric}' in ref_metrics and f'mrl_{metric}' in ref_metrics:
                    regular_score = ref_metrics[f'regular_{metric}']
                    mrl_score = ref_metrics[f'mrl_{metric}']
                    difference = mrl_score - regular_score
                    percent_improvement = (difference / regular_score) * 100 if regular_score > 0 else 0
                    
                    f.write(f"| {metric.upper()} | {regular_score:.4f} | {mrl_score:.4f} | {difference:.4f} | {percent_improvement:.2f}% |\n")
            
            # Overall comparison
            f.write("\n### Overall Comparison\n\n")
            
            # Count how many metrics MRL is better in
            mrl_better_count = sum(1 for metric in ['rouge1', 'rouge2', 'rougeL', 'bleu', 'bert_f1'] 
                                if f'regular_{metric}' in ref_metrics and f'mrl_{metric}' in ref_metrics and
                                ref_metrics[f'mrl_{metric}'] > ref_metrics[f'regular_{metric}'])
            total_metrics = sum(1 for metric in ['rouge1', 'rouge2', 'rougeL', 'bleu', 'bert_f1'] 
                             if f'regular_{metric}' in ref_metrics and f'mrl_{metric}' in ref_metrics)
            
            f.write(f"MRL embeddings performed better in {mrl_better_count} out of {total_metrics} metrics.\n\n")
            
            # Per-query analysis
            f.write("### Per-Query Analysis\n\n")
            f.write("Number of queries where MRL embeddings performed better:\n\n")
            
            for metric in ['rouge1', 'rouge2', 'rougeL', 'bleu', 'bert_f1']:
                if f'regular_{metric}' in with_ref.columns and f'mrl_{metric}' in with_ref.columns:
                    mrl_better_queries = sum(1 for _, row in with_ref.iterrows() 
                                            if row[f'mrl_{metric}'] > row[f'regular_{metric}'])
                    total_queries = len(with_ref)
                    percentage = (mrl_better_queries / total_queries) * 100
                    
                    f.write(f"- {metric.upper()}: {mrl_better_queries}/{total_queries} queries ({percentage:.2f}%)\n")
    
    logging.info(f"Summary report generated at {report_file}")

def generate_visualizations(df, avg_scores):
    """Generate visualizations of the metrics"""
    # Create directory for visualizations
    viz_dir = os.path.join(RESULTS_DIR, "visualizations")
    os.makedirs(viz_dir, exist_ok=True)
    
    # 1. Bar chart of answer lengths
    plt.figure(figsize=(10, 6))
    
    avg_regular_length = df['regular_length'].mean()
    avg_mrl_length = df['mrl_length'].mean()
    
    plt.bar(['Regular Embeddings', 'MRL Embeddings'], [avg_regular_length, avg_mrl_length])
    plt.ylabel('Average Answer Length (characters)')
    plt.title('Answer Length Comparison')
    plt.grid(True, axis='y', alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(os.path.join(viz_dir, 'answer_length_comparison.png'), dpi=300)
    plt.close()
    
    # 2. Histogram of similarity scores
    plt.figure(figsize=(10, 6))
    plt.hist(df['similarity_score'], bins=20, alpha=0.7)
    plt.xlabel('Semantic Similarity Score')
    plt.ylabel('Number of Queries')
    plt.title('Distribution of Semantic Similarity Between Regular and MRL Answers')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(os.path.join(viz_dir, 'similarity_distribution.png'), dpi=300)
    plt.close()
    
    # Filter to only queries with reference answers for reference-based visualizations
    with_ref = df[df['has_reference'] == True]
    
    if not with_ref.empty:
        # 3. Bar chart of average reference-based scores
        plt.figure(figsize=(12, 6))
        metrics = ['rouge1', 'rouge2', 'rougeL', 'bleu', 'bert_f1']
        available_metrics = [m for m in metrics if f'regular_{m}' in with_ref.columns and f'mrl_{m}' in with_ref.columns]
        
        if available_metrics:
            x = np.arange(len(available_metrics))
            width = 0.35
            
            regular_scores = [with_ref[f'regular_{m}'].mean() for m in available_metrics]
            mrl_scores = [with_ref[f'mrl_{m}'].mean() for m in available_metrics]
            
            plt.bar(x - width/2, regular_scores, width, label='Regular Embeddings')
            plt.bar(x + width/2, mrl_scores, width, label='MRL Embeddings')
            
            plt.xlabel('Metrics')
            plt.ylabel('Score')
            plt.title('Average Reference-Based Scores by Metric')
            plt.xticks(x, [m.upper() for m in available_metrics])
            plt.legend()
            plt.tight_layout()
            
            plt.savefig(os.path.join(viz_dir, 'average_ref_scores.png'), dpi=300)
            plt.close()
        
        # 4. Scatter plot comparing ROUGE-L scores if available
        if 'regular_rougeL' in with_ref.columns and 'mrl_rougeL' in with_ref.columns:
            plt.figure(figsize=(10, 8))
            plt.scatter(with_ref['regular_rougeL'], with_ref['mrl_rougeL'], alpha=0.7)
            
            # Add diagonal line
            max_val = max(with_ref['regular_rougeL'].max(), with_ref['mrl_rougeL'].max())
            plt.plot([0, max_val], [0, max_val], 'r--')
            
            plt.xlabel('Regular Embeddings ROUGE-L')
            plt.ylabel('MRL Embeddings ROUGE-L')
            plt.title('ROUGE-L Scores Comparison')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            plt.savefig(os.path.join(viz_dir, 'rouge_l_comparison.png'), dpi=300)
            plt.close()
        
        # 5. Box plot of score differences if reference metrics are available
        if available_metrics:
            plt.figure(figsize=(12, 6))
            
            # Calculate differences (MRL - Regular)
            differences = {}
            for metric in available_metrics:
                differences[metric.upper()] = with_ref[f'mrl_{metric}'] - with_ref[f'regular_{metric}']
            
            plt.boxplot(list(differences.values()), labels=list(differences.keys()))
            plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)
            plt.ylabel('Score Difference (MRL - Regular)')
            plt.title('Distribution of Score Differences')
            plt.grid(True, axis='y', alpha=0.3)
            plt.tight_layout()
            
            plt.savefig(os.path.join(viz_dir, 'score_differences.png'), dpi=300)
            plt.close()
    
    logging.info(f"Visualizations saved to {viz_dir}")

if __name__ == "__main__":
    # Check for required packages
    try:
        import nltk
        nltk.download('punkt', quiet=True)
    except ImportError:
        logging.warning("NLTK not found. Installing...")
        import subprocess
        subprocess.check_call(["pip", "install", "nltk"])
        import nltk
        nltk.download('punkt', quiet=True)
    
    try:
        import rouge_score
    except ImportError:
        logging.warning("rouge_score not found. Installing...")
        import subprocess
        subprocess.check_call(["pip", "install", "rouge_score"])
    
    try:
        import bert_score
    except ImportError:
        logging.warning("bert_score not found. Installing...")
        import subprocess
        subprocess.check_call(["pip", "install", "bert_score"])
    
    # Run the evaluation
    evaluate_answers()
